#!/usr/bin/env python

"""
Profile STR polymorphism error rates and genotype loci summarized by
msitools.

IMPORTANT: error rates are generated by considering hemizygous sex
chromosomes, so the genotyper will only have an empirical error distribution
when:
    1. the sample is male and
    2. data exists for the X and Y chromosomes
This means that one CANNOT parallelize genotyping by chromosome.  That could
be enabled by separating the error profile logic into a separate program, but
it has not yet been necessary for me.
"""

import sys
from argparse import ArgumentParser
from strlocusiterator import STRLocusIterator
from collections import defaultdict
from itertools import product
from numpy import array, prod
from scipy.stats import binom_test
from operator import itemgetter


unit_to_int = { 'mono':1, 'di':2, 'tri':3, 'tetra':4 }


def profile_error_distn(lw_params, is_single_cell=False):
    """Create a profile of likely erroneous read -> STR allele mappings.
    The method works by examining STR polymorphisms on hemizygous sex
    chromosomes (e.g., depends on the subject being male). At each locus:
      1. the primary allele is determined by the allele with the highest
         number of supporting reads.
      2. all reads supporting a non-primary allele are considered to be
         errors.  Note that in non-single cell samples, this assumption may
         be false since there may exist several distinct alleles in different
         cell subpopulations.  To handle this case, we provide a very simple
         binomial model that allows for up to 2 alleles to be treated as non-
         error mode alleles if the p-value of a binomial test is > 0.05.
    This process produces 4 counts: the number of non-error-mode reads and
    loci and the number of error-mode reads and loci.  These counts are
    further subdivided by STR unit type (mono, di, ...) and length of the
    reference allele.  (They could even further be divided by length of the
    alternate allele, since +1/-1 polymorphisms should be more likely to
    occur by random error than +2/-2 polymorphisms and so on.)

    If my understanding is correct, this should mostly explain variability
    due to polymerase slippage, but other effects will be captured.

    Allow only a single primary allele by disabling the binomial model in
    estimate_error if `is_single_cell`=True."""
    errors = defaultdict(lambda: defaultdict(lambda: array([0, 0, 0, 0])))

    with STRLocusIterator(hemizygous_only=True, **lw_params) as locus_f:
        for (chrom, start, end, reflen, unit, region, flank1, flank2, seq, reads) in locus_f:
            summaries = summarize_alleles(reads, reflen)
            errors[unit][reflen] += \
                estimate_error(summaries, use_binom=not is_single_cell)

    # dict(unit -> dict(reflen ->
    # (total reads, likely erroneous reads, total loci, likely erroneous loci)))
    return errors


def get_error_estimate(errors, unit, reflen, default=0.01):
    # This should rarely happen, but handle it when it does
    if not unit in errors.keys():
        return default

    if reflen in errors[unit].keys():
        profile = errors[unit][reflen]
    else:
        # We may not have seen any instances of a particular reference length
        # when profiling the sex chromosomes.  Use the nearest reference length.
        diffs = [ abs(reflen - l) for l in errors[unit].keys() ]
        m = min(diffs)
        nearest = [ l for l in errors[unit].keys() if abs(reflen - l) == m ][0]
        profile = errors[unit][nearest]

    tot_reads, err_reads = profile[0:2]
    return max(default, float(err_reads)/tot_reads)


def save_error_distn(error_distn_file, errors):
    with open(error_distn_file, 'w') as f:
        f.write("unit\treflen\terror_reads\ttotal_reads\tpercent_error_reads\terror_loci\ttotal_loci\tpercent_error_loci\n")
        for unit in sorted(errors.keys(), key=lambda x: unit_to_int[x]):
            for reflen in sorted(errors[unit].keys()):
                x = errors[unit][reflen]
                f.write("%s\t%d\t%d\t%d\t%.3f\t%d\t%d\t%.3f\n" % \
                        (unit, reflen, x[1], x[0], float(x[1])/x[0],
                         x[3], x[2], float(x[3])/x[2]))


# The function from itertools in python 2.7+
def combinations_with_replacement(iterable, r):
    pool = tuple(iterable)
    n = len(pool)
    for indices in product(range(n), repeat=r):
        if sorted(indices) == list(indices):
            yield tuple(pool[i] for i in indices)


def summarize_alleles(reads, reflen):
    """`reads` is a list of (obs len, strand, mapq) tuples each describing
    a single read.
    `reflen` is the length of the STR in the reference genome."""
    # Map: observed len -> (num reads, num forward strand, sum mapq)
    metrics = defaultdict(lambda: array([0, 0, 0]))
    for obs, strand, mapq in reads:
        metrics[obs - reflen] += array([ 1, 1 if strand == '+' else 0, mapq ])

    return dict((k, (n, float(nforward) / n, float(summapq) / n))
                for k, (n, nforward, summapq) in metrics.iteritems())


def genotype_locus(alleles, err=0.01):
    """`err` is probability of an STR polymorphism error in a single read,
    conditioned on the repeat unit length (mono, di, ...) and the reference
    length of the STR.  Shorter repeat units and longer reference lengths
    should correspond to increased error rates."""
    # Try every possible selection of 2 alleles from the set
    gts = list(combinations_with_replacement(alleles.keys(), 2))

    def hap_prob(x, Nalleles, haptype, err):
        return 1 - err if x == haptype else err/(Nalleles - 1)

    def dip_prob(alleles, hapA, hapB, err):
        return prod([ (hap_prob(k, len(alleles), hapA, err)/2 + \
                       hap_prob(k, len(alleles), hapB, err)/2)**n
                      for k, (n, _1, _2) in alleles.iteritems() ])

    probs = [ dip_prob(alleles, a, b, err) for a, b in gts ]

    best = gts[probs.index(max(probs))]
    if best[0] == best[1]:
        if best[0] == 0:
            call = 'ref'
        else:
            call = 'hom'
    else:
        call = 'het'

    return (call, max(probs), str(best[0]) + "/" + str(best[1]))


def genotype(lw_params, filter_metrics_file, errors):
    """Genotype all loci in the STR locus file specified by lw_params.  Write
    the results to standard output and optionally write the filter metrics to
    `filter_metrics_file`."""

    with STRLocusIterator(**lw_params) as locus_f:
        print("chr\tstart\tend\tref_len\tunit\tregion\tflank1\tflank2\tsequence\traw_alleles\tcall\tgenotype\tpval\tallele_summaries")
        for (chrom, start, end, reflen, unit, region, flank1, flank2, seq, reads) in locus_f:
            summaries = summarize_alleles(reads, reflen)
            alleles = " ".join("%d:%d,%.2f,%.2f" % ((k,) + v)
                               for k, v in summaries.items())
            err = get_error_estimate(errors, unit, reflen)
            call, pval, gt = genotype_locus(summaries, err=err)
            print("%s\t%d\t%d\t%d\t%s\t%s\t%s\t%s\t%s\t%d\t%s\t%s\t%.5g\t%s" %
                  (chrom, start, end, reflen, unit, region, flank1, flank2,
                   seq, len(summaries), call, gt, pval, alleles))

        if filter_metrics_file:
            with open(filter_metrics_file, 'w') as f:
                for (description, value) in locus_f.filter_metrics():
                    f.write("%s\t%d\n" % (description, value))

                for (description, hist) in locus_f.hist_metrics():
                    f.write(description + "\n")
                    for k in sorted(hist.keys()):
                        f.write("%s\t%d\n" % (k, hist[k]))


def estimate_error(summaries, use_binom=False, binom_threshold=0.05):
    # Sort by highest number of reads
    # Summaries is a dict of tuples: lendiff -> nreads, fracforward, meanmapq
    # We don't care about anything but the number of reads now.
    nreads = sorted([ x[0] for x in summaries.values() ], reverse=True)

    # reads supporting any alleles other than the top 2 are always
    # considered errors
    likely_stutter = sum(nreads[2:])
    if use_binom:
        # Use a simple but arbitrary binomial model to distinguish between
        # heterozygous loci and polymerase stutter
        if len(nreads) > 1:
            # Get the "best" two allele lens
            obs1, obs2 = nreads[0:2]
            if binom_test(obs1, n=obs1 + obs2, p=0.5) <= binom_threshold:
                # Reject hypothesis that each allele is equally likely: stutter
                likely_stutter += obs2    # obs1 is never considered stutter
                #print("stutter: " + summaries)
    else:
        if len(nreads) > 1:
            likely_stutter += nreads[1]

    return array([ sum(nreads), likely_stutter, 1, 1 if likely_stutter else 0 ])


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument('--error-distn-file', metavar='file', type=str,
        required=True,
        help="File to which STR polymorphism error rates will be written.")
    parser.add_argument('--filter-metrics-file', metavar='file', type=str,
        help='File to store metrics related to locus and read filtering.')
    parser.add_argument('--single-cell', action='store_true', default=False,
        help="Library was generated from a single cell.  Disables the " \
             "binomial model for >1 primary alleles.")
    STRLocusIterator.add_parser_args(parser)
    args = parser.parse_args()

    # Many of the command line args are STRLocusWalker parameters
    lw_params = dict(vars(args))
    del(lw_params['error_distn_file'])
    del(lw_params['single_cell'])
    del(lw_params['filter_metrics_file'])

    # Step 1. Generate an STR length polymorphism error profile and save it.
    errors = profile_error_distn(lw_params, is_single_cell=args.single_cell)
    save_error_distn(args.error_distn_file, errors)

    # Step 2. Now that we have an empirical distribution of STR polymorphism
    # error rates, genotype the loci.  The results are printed to stdout.
    genotype(lw_params, args.filter_metrics_file, errors)
